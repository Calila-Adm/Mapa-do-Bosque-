# Planejamento - ImplementaÃ§Ã£o do Chat LLM AvanÃ§ado

Este documento detalha o plano completo para implementar um **Chat LLM Contextual** no canto inferior esquerdo de cada notebook, similar ao disponÃ­vel na versÃ£o premium do Briefer.

---

## ğŸ¯ **VisÃ£o Geral do Recurso**

### **Funcionalidades Desejadas:**
- ğŸ’¬ **Chat flutuante** no canto inferior esquerdo dos notebooks
- ğŸ§  **LLM avanÃ§ada** com melhor compreensÃ£o de contexto
- ğŸ“Š **Contexto de Data Sources** automÃ¡tico
- ğŸ” **ExecuÃ§Ã£o de queries** direto do chat
- ğŸ’¡ **SugestÃµes inteligentes** baseadas no notebook atual
- ğŸ“ **HistÃ³rico de conversas** persistente

### **AnÃ¡lise do Sistema Atual:**
O projeto **jÃ¡ possui** infraestrutura bÃ¡sica de AI:
- âœ… **API Python** com FastAPI em `/ai/`
- âœ… Hook `useAI` para comunicaÃ§Ã£o com AI
- âœ… API endpoints para SQL/Python edit
- âœ… Streaming com FastAPI StreamingResponse
- âœ… IntegraÃ§Ã£o com LangChain
- âŒ **NÃ£o possui** chat conversacional
- âŒ **NÃ£o possui** integraÃ§Ã£o com Claude/Anthropic
- âŒ **NÃ£o possui** contexto de data sources

---

## ğŸ—ï¸ **Arquitetura Proposta**

### **1. Frontend - Componente de Chat**

```typescript
// Estrutura de arquivos:
/apps/web/src/components/AIChat/
â”œâ”€â”€ index.tsx              // Componente principal
â”œâ”€â”€ ChatWindow.tsx         // Janela do chat
â”œâ”€â”€ ChatMessage.tsx        // Mensagem individual
â”œâ”€â”€ ChatInput.tsx          // Input do usuÃ¡rio
â”œâ”€â”€ ChatContext.tsx        // Context API para estado
â”œâ”€â”€ FloatingButton.tsx     // BotÃ£o flutuante
â””â”€â”€ hooks/
    â”œâ”€â”€ useChat.ts         // Hook principal do chat
    â”œâ”€â”€ useDataContext.ts  // Hook para contexto de dados
    â””â”€â”€ useChatHistory.ts  // Hook para histÃ³rico
```

### **2. Backend - API do Chat (Python)**

```python
# Estrutura de arquivos - Expandindo a pasta /ai existente:
/ai/
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py             # FastAPI app existente
â”‚   â”œâ”€â”€ llms.py            # LLMs config existente
â”‚   â””â”€â”€ chains/
â”‚       â”œâ”€â”€ stream/        # Chains existentes
â”‚       â””â”€â”€ chat/          # NOVO - Chat chains
â”‚           â”œâ”€â”€ __init__.py
â”‚           â”œâ”€â”€ conversation.py  # Chain de conversaÃ§Ã£o
â”‚           â”œâ”€â”€ context.py       # Contexto de dados
â”‚           â””â”€â”€ prompts.py       # System prompts
â”œâ”€â”€ services/              # NOVO - ServiÃ§os
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_source.py    # Contexto de data sources
â”‚   â”œâ”€â”€ schema_cache.py   # Cache LRU para schemas
â”‚   â””â”€â”€ history.py        # Gerenciamento de histÃ³rico
â””â”€â”€ requirements.txt       # Adicionar anthropic SDK
```

---

## ğŸ“‹ **Plano de ImplementaÃ§Ã£o Detalhado**

### **FASE 1: Chat BÃ¡sico (3-4 dias)**

#### **1.1 Criar Componente de Chat Flutuante**

**Arquivo:** `/apps/web/src/components/AIChat/FloatingButton.tsx`
```tsx
import { ChatBubbleLeftRightIcon } from '@heroicons/react/24/outline'
import { useState } from 'react'
import ChatWindow from './ChatWindow'

export default function FloatingButton() {
  const [isOpen, setIsOpen] = useState(false)
  
  return (
    <>
      {/* BotÃ£o Flutuante */}
      <button
        onClick={() => setIsOpen(!isOpen)}
        className="fixed bottom-6 left-6 z-50 
                   bg-primary-500 hover:bg-primary-600 
                   text-white rounded-full p-4 shadow-lg
                   transition-all transform hover:scale-110"
      >
        <ChatBubbleLeftRightIcon className="h-6 w-6" />
      </button>
      
      {/* Janela do Chat */}
      {isOpen && (
        <ChatWindow onClose={() => setIsOpen(false)} />
      )}
    </>
  )
}
```

#### **1.2 Implementar Janela do Chat com Error Boundary**

**Arquivo:** `/apps/web/src/components/AIChat/ChatWindow.tsx`
```tsx
import { ErrorBoundary } from 'react-error-boundary'

function ChatWindowContent({ onClose }) {
  const [messages, setMessages] = useState([])
  const [input, setInput] = useState('')
  const { sendMessage, loading } = useChat()
  
  return (
    <div className="fixed bottom-20 left-6 z-50 
                    w-96 h-[600px] bg-white rounded-lg shadow-2xl
                    border border-gray-200 flex flex-col">
      {/* Header */}
      <div className="p-4 border-b flex justify-between items-center">
        <h3 className="font-semibold">AI Assistant</h3>
        <button onClick={onClose}>
          <XMarkIcon className="h-5 w-5" />
        </button>
      </div>
      
      {/* Messages */}
      <div className="flex-1 overflow-y-auto p-4">
        {messages.map((msg, i) => (
          <ChatMessage key={i} message={msg} />
        ))}
      </div>
      
      {/* Input com Debounce */}
      <ChatInput 
        value={input}
        onChange={setInput}
        onSend={() => sendMessage(input)}
        loading={loading}
        debounceMs={300} // Evita mÃºltiplas requisiÃ§Ãµes
      />
    </div>
  )
}

// Componente com Error Boundary
export default function ChatWindow({ onClose }) {
  return (
    <ErrorBoundary
      fallback={
        <div className="fixed bottom-20 left-6 z-50 w-96 p-4 bg-red-50 rounded-lg">
          <p className="text-red-600">Erro no chat. Por favor, recarregue a pÃ¡gina.</p>
        </div>
      }
      onReset={() => window.location.reload()}
    >
      <ChatWindowContent onClose={onClose} />
    </ErrorBoundary>
  )
}
```

#### **1.3 Adicionar ao Layout do Notebook**

**Modificar:** `/apps/web/src/components/PrivateDocumentPage.tsx`
```tsx
import FloatingAIChat from '@/components/AIChat/FloatingButton'

// Adicionar dentro do componente:
{showAIChat && (
  <FloatingAIChat 
    workspaceId={workspaceId}
    documentId={documentId}
  />
)}
```

---

### **FASE 2: Contexto de Data Sources (2-3 dias)**

#### **2.1 Criar ServiÃ§o de Contexto com Cache**

**Arquivo:** `/ai/services/schema_cache.py`
```python
from cachetools import TTLCache
import asyncio
from typing import Dict, Any, Optional
import httpx
import os

class DataSourceContext:
    def __init__(self):
        # Cache com TTL de 10 minutos e mÃ¡ximo de 100 schemas
        self.schema_cache = TTLCache(maxsize=100, ttl=600)
        self.api_base_url = os.getenv("API_BASE_URL", "http://localhost:3000")
    
    async def get_schema_context(self, data_source_id: str, auth_token: str) -> Dict[str, Any]:
        # Verificar cache primeiro
        if data_source_id in self.schema_cache:
            print(f"Schema cache hit for {data_source_id}")
            return self.schema_cache[data_source_id]
        
        # Buscar schemas via API interna
        async with httpx.AsyncClient() as client:
            response = await client.get(
                f"{self.api_base_url}/api/v1/datasources/{data_source_id}/schema",
                headers={"Authorization": f"Bearer {auth_token}"}
            )
            schema = response.json()
        
        formatted = {
            "tables": [
                {
                    "name": table["name"],
                    "columns": [
                        {
                            "name": col["name"],
                            "type": col["type"],
                            "nullable": col.get("nullable", True)
                        }
                        for col in table["columns"]
                    ]
                }
                for table in schema.get("tables", [])
            ]
        }
        
        # Salvar no cache
        self.schema_cache[data_source_id] = formatted
        
        return formatted
    
    async def get_sample_data(self, data_source_id: str, table_name: str, auth_token: str):
        # Buscar dados de exemplo
        query = f"SELECT * FROM {table_name} LIMIT 5"
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.api_base_url}/api/v1/datasources/{data_source_id}/query",
                headers={"Authorization": f"Bearer {auth_token}"},
                json={"query": query}
            )
            return response.json()
    
    def invalidate_cache(self, data_source_id: str):
        """Limpar cache quando schema mudar"""
        if data_source_id in self.schema_cache:
            del self.schema_cache[data_source_id]
```

#### **2.2 Hook para Contexto**

**Arquivo:** `/apps/web/src/components/AIChat/hooks/useDataContext.ts`
```typescript
export function useDataContext(workspaceId: string) {
  const [dataSources] = useDataSources(workspaceId)
  const [activeDataSource, setActiveDataSource] = useState(null)
  const [schema, setSchema] = useState(null)
  
  useEffect(() => {
    if (activeDataSource) {
      fetchSchema(activeDataSource.id).then(setSchema)
    }
  }, [activeDataSource])
  
  return {
    dataSources,
    activeDataSource,
    schema,
    setActiveDataSource
  }
}
```

---

### **FASE 3: IntegraÃ§Ã£o com LLM (3-4 dias)**

#### **3.1 API Endpoint do Chat com Claude (Python)**

**Arquivo:** `/ai/api/app.py` (adicionar ao existente)
```python
from anthropic import AsyncAnthropic
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import json

# Adicionar ao arquivo existente

class ChatMessage(BaseModel):
    role: str  # 'user' ou 'assistant'
    content: str

class ChatInputData(BaseModel):
    message: str
    history: List[ChatMessage]
    context: Dict[str, Any]  # Contexto do notebook/data source
    workspaceId: str
    documentId: str
    dataSourceId: Optional[str] = None

@app.post("/v1/stream/chat")
async def v1_stream_chat(data: ChatInputData, _ = Depends(get_current_username)):
    # Inicializar Claude
    anthropic = AsyncAnthropic(
        api_key=config("ANTHROPIC_API_KEY")
    )
    
    # Importar serviÃ§os
    from services.schema_cache import DataSourceContext
    from api.chains.chat.prompts import build_system_prompt
    
    # Obter contexto se houver data source
    context_service = DataSourceContext()
    schema_context = None
    
    if data.dataSourceId:
        schema_context = await context_service.get_schema_context(
            data.dataSourceId, 
            # Token seria passado no header
            request.headers.get("Authorization", "").replace("Bearer ", "")
        )
    
    # Construir system prompt com contexto
    system_prompt = build_system_prompt(data.context, schema_context)
    
    # Formatar histÃ³rico para Claude
    messages = [
        {
            "role": msg.role if msg.role in ["user", "assistant"] else "assistant",
            "content": msg.content
        }
        for msg in data.history
    ]
    messages.append({"role": "user", "content": data.message})
    
    async def generate():
        # Stream com Claude
        async with anthropic.messages.stream(
            model="claude-3-5-sonnet-20241022",
            system=system_prompt,
            messages=messages,
            max_tokens=4096,
            temperature=0.7
        ) as stream:
            async for chunk in stream:
                if chunk.type == "content_block_delta":
                    yield json.dumps({"text": chunk.delta.text}) + "\n"
    
    return StreamingResponse(generate(), media_type="text/plain")
```

#### **3.2 System Prompt Inteligente**

**Arquivo:** `/ai/api/chains/chat/prompts.py`
```python
from typing import Dict, Any, Optional

def build_system_prompt(context: Dict[str, Any], schema_context: Optional[Dict[str, Any]] = None) -> str:
    """Construir system prompt com contexto do notebook e data source"""
    
    prompt = """You are an advanced data analysis assistant integrated into Mapa do Bosque.

CONTEXT:
- Current notebook: {notebook_name}
- Active data source: {data_source_name}
- Database type: {database_type}

{schema_section}

CAPABILITIES:
1. Write and optimize SQL queries
2. Explain data structures
3. Suggest analyses
4. Debug errors
5. Create Python code for data analysis

RULES:
- Be concise but thorough
- Always consider the data context
- Suggest best practices
- Format code properly
- Explain complex concepts simply
- Use Brazilian Portuguese when appropriate

Current user's question follows:
"""
    
    # Adicionar informaÃ§Ãµes de schema se disponÃ­vel
    schema_section = ""
    if schema_context and schema_context.get("tables"):
        schema_section = "AVAILABLE TABLES:\n"
        for table in schema_context["tables"]:
            columns = ", ".join([col["name"] for col in table["columns"]])
            schema_section += f"- {table['name']}: {columns}\n"
    
    return prompt.format(
        notebook_name=context.get("notebookName", "Untitled"),
        data_source_name=context.get("dataSource", {}).get("name", "None"),
        database_type=context.get("dataSource", {}).get("type", "Unknown"),
        schema_section=schema_section
    )
```

---

### **FASE 4: ExecuÃ§Ã£o de Queries (2-3 dias)**

#### **4.1 Detectar e Executar SQL**

**Arquivo:** `/apps/web/src/components/AIChat/hooks/useQueryExecution.ts`
```typescript
export function useQueryExecution() {
  const detectSQL = (message: string) => {
    const sqlPattern = /```sql\n([\s\S]*?)\n```/g
    const matches = message.matchAll(sqlPattern)
    return Array.from(matches).map(m => m[1])
  }
  
  const executeQuery = async (sql: string, dataSourceId: string) => {
    try {
      const result = await runQuery(dataSourceId, sql)
      return {
        success: true,
        data: result,
        rowCount: result.length
      }
    } catch (error) {
      return {
        success: false,
        error: error.message
      }
    }
  }
  
  return { detectSQL, executeQuery }
}
```

#### **4.2 UI para Resultados**

```tsx
// Componente para mostrar resultados de query
function QueryResult({ result }) {
  if (!result.success) {
    return (
      <div className="bg-red-50 p-3 rounded">
        <p className="text-red-600">Erro: {result.error}</p>
      </div>
    )
  }
  
  return (
    <div className="bg-gray-50 p-3 rounded">
      <p className="text-sm text-gray-600">
        {result.rowCount} linhas retornadas
      </p>
      <Table data={result.data} />
      <button className="mt-2 text-blue-600 text-sm">
        Adicionar ao notebook
      </button>
    </div>
  )
}
```

---

### **FASE 5: HistÃ³rico e PersistÃªncia (1-2 dias)**

#### **5.1 Salvar HistÃ³rico no Banco**

**Schema:** `chat_history`
```sql
CREATE TABLE chat_history (
  id UUID PRIMARY KEY,
  workspace_id UUID NOT NULL,
  document_id UUID NOT NULL,
  user_id UUID NOT NULL,
  message TEXT NOT NULL,
  role VARCHAR(10) NOT NULL, -- 'user' ou 'assistant'
  metadata JSONB,
  created_at TIMESTAMP DEFAULT NOW()
);
```

#### **5.2 Hook para HistÃ³rico**

```typescript
export function useChatHistory(documentId: string) {
  const [history, setHistory] = useState([])
  
  useEffect(() => {
    // Carregar histÃ³rico do banco
    loadHistory(documentId).then(setHistory)
  }, [documentId])
  
  const addMessage = async (message, role) => {
    const msg = { message, role, timestamp: new Date() }
    setHistory(prev => [...prev, msg])
    await saveMessage(documentId, msg)
  }
  
  return { history, addMessage }
}
```

---

## ğŸ¨ **UI/UX Design**

### **Estados do Chat:**

1. **Minimizado:** Apenas botÃ£o flutuante
2. **Aberto:** Janela de chat completa
3. **Loading:** Indicador de digitaÃ§Ã£o "AI estÃ¡ pensando..."
4. **Erro:** Mensagem de erro com retry

### **Atalhos de Teclado:**
- `Cmd/Ctrl + K` - Abrir/fechar chat
- `Cmd/Ctrl + Enter` - Enviar mensagem
- `Esc` - Fechar chat

### **Features AvanÃ§adas:**
- **Markdown Support** - Renderizar markdown nas respostas
- **Syntax Highlighting** - Highlight de cÃ³digo
- **Copy Button** - Copiar cÃ³digo/queries
- **Export Chat** - Exportar conversa

---

## ğŸ”§ **ConfiguraÃ§Ã£o e VariÃ¡veis**

### **VariÃ¡veis de Ambiente NecessÃ¡rias:**

```bash
# LLM Configuration - Apenas Anthropic Claude
ANTHROPIC_API_KEY=sk-ant-...
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# API Configuration (Python)
API_BASE_URL=http://localhost:3000
BASIC_AUTH_USERNAME=briefer
BASIC_AUTH_PASSWORD=secretpassword

# Feature Flags
ENABLE_AI_CHAT=true
AI_CHAT_MAX_TOKENS=4096  # Claude suporta mais tokens
AI_CHAT_TEMPERATURE=0.7

# Rate Limiting
AI_CHAT_RATE_LIMIT=100 # requests per hour
```

### **ConfiguraÃ§Ã£o do Workspace:**

```typescript
// Adicionar em settings
{
  aiChat: {
    enabled: true,
    model: 'claude-3-5-sonnet-20241022',
    includeDataContext: true,
    maxHistorySize: 50
  }
}
```

---

## ğŸ“Š **Estimativa de Tempo**

### **Cronograma Detalhado:**

| Fase | DescriÃ§Ã£o | Tempo | Complexidade |
|------|-----------|-------|--------------|
| **Fase 1** | Chat BÃ¡sico UI | 3-4 dias | MÃ©dia |
| **Fase 2** | Contexto Data Sources | 2-3 dias | Alta |
| **Fase 3** | IntegraÃ§Ã£o LLM | 3-4 dias | Alta |
| **Fase 4** | ExecuÃ§Ã£o Queries | 2-3 dias | MÃ©dia |
| **Fase 5** | HistÃ³rico | 1-2 dias | Baixa |
| **Testes** | Testes e Ajustes | 2-3 dias | MÃ©dia |
| **TOTAL** | **ImplementaÃ§Ã£o Completa** | **13-19 dias** | **Alta** |

### **MVP Simplificado (5-7 dias):**
- Chat bÃ¡sico com UI
- IntegraÃ§Ã£o Claude simples
- Sem contexto de data sources
- Sem execuÃ§Ã£o de queries
- HistÃ³rico apenas na sessÃ£o

---

## ğŸš€ **ImplementaÃ§Ã£o Passo a Passo**

### **Semana 1: Foundation**
1. âœ… Criar componente de chat flutuante
2. âœ… Implementar UI bÃ¡sica do chat
3. âœ… Conectar com API existente
4. âœ… Adicionar ao notebook

### **Semana 2: Intelligence**
5. âœ… Implementar contexto de data sources
6. âœ… Criar system prompts inteligentes
7. âœ… Integrar com Claude
8. âœ… Stream de respostas

### **Semana 3: Features**
9. âœ… Detectar e executar queries
10. âœ… Adicionar histÃ³rico persistente
11. âœ… Implementar atalhos
12. âœ… Testes e refinamentos

---

## ğŸ¯ **Resultado Final Esperado**

### **Funcionalidades Entregues:**
- ğŸ’¬ Chat flutuante em todos os notebooks
- ğŸ§  IA com contexto completo dos dados
- ğŸ” ExecuÃ§Ã£o de queries direto do chat
- ğŸ“ HistÃ³rico persistente de conversas
- âš¡ Respostas rÃ¡pidas e precisas
- ğŸ¨ UI moderna e intuitiva

### **BenefÃ­cios:**
- **Produtividade:** AnÃ¡lises mais rÃ¡pidas
- **Aprendizado:** IA ensina SQL/Python
- **Debugging:** Ajuda a resolver erros
- **ExploraÃ§Ã£o:** Descoberta de insights

---

## ğŸ“ **Notas de ImplementaÃ§Ã£o**

### **Prioridades:**
1. **MVP primeiro** - Chat bÃ¡sico funcionando
2. **Contexto depois** - Adicionar inteligÃªncia
3. **Features por Ãºltimo** - Polimento e extras

### **Cuidados:**
- Rate limiting para evitar custos altos
- **Cache de contexto** para schemas de banco (10 min TTL)
- **Debounce no input** (300ms) para evitar requisiÃ§Ãµes excessivas
- **Error boundaries** em componentes React
- ValidaÃ§Ã£o de queries antes de executar
- SanitizaÃ§Ã£o de inputs do usuÃ¡rio
- Claude gerencia tokens automaticamente (200k context window)

### **Melhorias Futuras:**
- Voice input/output
- Multi-modal (imagens)
- Agentes autÃ´nomos
- Fine-tuning do modelo

---

## âœ… **Checklist de ImplementaÃ§Ã£o**

### **Frontend (React/TypeScript):**
- [ ] Criar estrutura de pastas em `/apps/web/src/components/AIChat/`
- [ ] Implementar componente flutuante
- [ ] Criar janela de chat com Error Boundary
- [ ] Adicionar debounce no input (300ms)
- [ ] Conectar com API Python via fetch/axios
- [ ] Implementar hook useChat

### **Backend (Python/FastAPI):**
- [ ] Expandir estrutura em `/ai/`
- [ ] Adicionar anthropic ao requirements.txt
- [ ] Criar endpoint `/v1/stream/chat` no app.py
- [ ] Implementar serviÃ§o de cache com cachetools
- [ ] Criar chains de conversaÃ§Ã£o
- [ ] Integrar Anthropic SDK para Claude
- [ ] Adicionar contexto de dados com cache TTL
- [ ] Implementar execuÃ§Ã£o de queries

### **Geral:**
- [ ] Adicionar histÃ³rico no PostgreSQL
- [ ] Testes E2E
- [ ] DocumentaÃ§Ã£o
- [ ] Deploy

**Status:** ğŸ“‹ **PLANEJAMENTO COMPLETO - PRONTO PARA IMPLEMENTAÃ‡ÃƒO**